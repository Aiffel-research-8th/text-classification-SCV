{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e55578",
   "metadata": {},
   "source": [
    "## 0. 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b880b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import json\n",
    "from soynlp.normalizer import *\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868b179",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed6af54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>일반 대화</td>\n",
       "      <td>나 머리가 아침부터 아프네.\\n타이레놀 먹어봐\\n나는 타이레놀 만성이 되어서 안 들...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>그 새끼 눈에 띄기만 하면 죽여버릴거니까 그런줄알아.\\n아니 그래도 대화는 해봐야죠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>니가 그러고도 친구야?\\n 그러는 너는?\\n 헛소문 좀 퍼트리지마 \\n 헛소문이라니...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>나 어제 너가 학원에서 선생님 지갑에서 돈 훔치는 거 봤어.\\n어.? 아. \\n처음...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>일반 대화</td>\n",
       "      <td>**아 오늘까지 과제 제출이라며. 했어?\\n아니 아직 안했어ㅠㅠ\\n응? 어떡할려고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6908</th>\n",
       "      <td>일반 대화</td>\n",
       "      <td>오늘 국민지원금으로 고기 파티했습니다.\\n와 정말 맛있었겠어요\\n정말 좋으셨겠어요\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>야. 너는 맨날 똑같은 옷만 입고 다니냐?\\n아니야.\\n더러워 진짜!\\n냄새도 나냐...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>씨 뭐해? \\n아 저 지금 일 하는 중입니다\\n바쁜가 보네 내가 접때 맡긴 건 다 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6911</th>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>안녕하세요 고객님 편하게 입어보세요\\n네 언니 이거 입어봐도되요?\\n아 고객님 이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6912</th>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>서준아 나 피시방 모자란대 5000원만 주라\\n 나도 내꺼 낼꺼 밖에 없는데\\n 너...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6913 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            class                                       conversation\n",
       "idx                                                                 \n",
       "0           일반 대화  나 머리가 아침부터 아프네.\\n타이레놀 먹어봐\\n나는 타이레놀 만성이 되어서 안 들...\n",
       "1           협박 대화  그 새끼 눈에 띄기만 하면 죽여버릴거니까 그런줄알아.\\n아니 그래도 대화는 해봐야죠...\n",
       "2           협박 대화  니가 그러고도 친구야?\\n 그러는 너는?\\n 헛소문 좀 퍼트리지마 \\n 헛소문이라니...\n",
       "3           갈취 대화  나 어제 너가 학원에서 선생님 지갑에서 돈 훔치는 거 봤어.\\n어.? 아. \\n처음...\n",
       "4           일반 대화  **아 오늘까지 과제 제출이라며. 했어?\\n아니 아직 안했어ㅠㅠ\\n응? 어떡할려고 ...\n",
       "...           ...                                                ...\n",
       "6908        일반 대화  오늘 국민지원금으로 고기 파티했습니다.\\n와 정말 맛있었겠어요\\n정말 좋으셨겠어요\\...\n",
       "6909    기타 괴롭힘 대화  야. 너는 맨날 똑같은 옷만 입고 다니냐?\\n아니야.\\n더러워 진짜!\\n냄새도 나냐...\n",
       "6910  직장 내 괴롭힘 대화  씨 뭐해? \\n아 저 지금 일 하는 중입니다\\n바쁜가 보네 내가 접때 맡긴 건 다 ...\n",
       "6911    기타 괴롭힘 대화  안녕하세요 고객님 편하게 입어보세요\\n네 언니 이거 입어봐도되요?\\n아 고객님 이 ...\n",
       "6912        갈취 대화  서준아 나 피시방 모자란대 5000원만 주라\\n 나도 내꺼 낼꺼 밖에 없는데\\n 너...\n",
       "\n",
       "[6913 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path =\"~/aiffel/dktc/data2/train5.csv\"\n",
    "train_data = pd.read_csv(train_data_path,index_col=0)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad387d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2963\n",
      "3950\n"
     ]
    }
   ],
   "source": [
    "# 일반 대화 2000개 분리\n",
    "\n",
    "normal_data = train_data[train_data[\"class\"] == \"일반 대화\"]\n",
    "not_normal_data = train_data[train_data[\"class\"] != \"일반 대화\"]\n",
    "print(len(normal_data))\n",
    "print(len(not_normal_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4669f1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4950\n",
      "1963\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.concat([not_normal_data, normal_data[:1000]], axis=\"rows\")\n",
    "normal_data = normal_data[1000:]\n",
    "print(len(train_data))\n",
    "print(len(normal_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc559a2a",
   "metadata": {},
   "source": [
    "## 2. 데이터 준비 (Data preparation)\n",
    "### 2.1-1 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5558ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # synolp\n",
    "    emoticon_normalize(sentence)\n",
    "    repeat_normalize(sentence)\n",
    "    #sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    # base preprocess\n",
    "    sentence = re.sub(r'([^a-zA-Zㄱ-ㅎ가-힣?.!,])', \" \", sentence)\n",
    "    sentence = re.sub(r'!+', '!', sentence)\n",
    "    sentence = re.sub(r'\\?+', '?', sentence)\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # 엔터 구분 (\\n)\n",
    "    sentence = sentence.replace(\"\\n\", \"<EOL>\")\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20fce76",
   "metadata": {},
   "source": [
    "### 2.1-2 전처리 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b825daf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4950/4950 [00:01<00:00, 3456.11it/s]\n",
      "100%|██████████| 1963/1963 [00:00<00:00, 3334.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# 학습할 문장이 담길 배열\n",
    "sentences = []\n",
    "\n",
    "for val in tqdm(train_data['conversation']):\n",
    "    sentences.append(preprocess_sentence(val))\n",
    "    \n",
    "normal_sentences = []\n",
    "\n",
    "for val in tqdm(normal_data['conversation']):\n",
    "    normal_sentences.append(preprocess_sentence(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1df119",
   "metadata": {},
   "source": [
    "### 2.2 최대 길이 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d0ab2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a90df",
   "metadata": {},
   "source": [
    "### 2.3 class(label) 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0155ba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4950\n",
      "1963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "CLASS_NAMES = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화','일반 대화']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(CLASS_NAMES)\n",
    "\n",
    "train_data['class'] = encoder.transform(train_data['class'])\n",
    "labels = train_data['class']\n",
    "\n",
    "normal_data['class'] = encoder.transform(normal_data['class'])\n",
    "normal_labels = normal_data[\"class\"]\n",
    "\n",
    "print(len(labels))\n",
    "print(len(normal_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "20a63a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: {'협박 대화': 4, '갈취 대화': 0, '직장 내 괴롭힘 대화': 3, '기타 괴롭힘 대화': 1, '일반 대화': 2}\n"
     ]
    }
   ],
   "source": [
    "class_mapping = {class_name: encoder.transform([class_name])[0] for class_name in CLASS_NAMES}\n",
    "print(\"Class mapping:\", class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "543db31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "    \n",
    "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "    \n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "        # input_id는 워드 임베딩을 위한 문장의 정수 인코딩\n",
    "        input_id = tokenizer.encode(example, \n",
    "                                    max_length=max_seq_len, \n",
    "                                    pad_to_max_length=True,\n",
    "                                   )\n",
    "        \n",
    "        # attention_mask는 실제 단어가 위치하면 1, 패딩의 위치에는 0인 시퀀스\n",
    "        padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "        attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "        \n",
    "        # token_type_id은 세그먼트 인코딩\n",
    "        token_type_id = [0] * max_seq_len\n",
    "        \n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(label)\n",
    "    \n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    \n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "    \n",
    "    return (input_ids, attention_masks, token_type_ids), data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bce20f",
   "metadata": {},
   "source": [
    "### 2.4 train-val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "906c1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2d300b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data augmentation:  3960\n",
      "after data augmentation:  9480\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3709</th>\n",
       "      <td>대리님 혹시 자료 보내주실 수 있을까요 ? 자료가 부족해서요 네 ? 나 알맞게 보냈...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5698</th>\n",
       "      <td>김회장 ! 오랜만이야 . 내가 놀라운 소식을 하나 들고 왔는데 말이야 . 지금 바쁘...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>또 뭘 처먹냐 뭐가 . 그만 좀 쳐먹어 씹돼지야 왜그래 . 너 걸어다닐때마다 바닥이...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>지금 뭐하는건가 ? 꼼짝마 손들어 내가 그럴 것 같니 ? 딸에게 독약이라도 줄 거야...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>내가 술 좀 그만 먹으라고 했지 ? 죽을래 ? 술을 먹든 말든 신경 꺼 네가 술 먹...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>저 책임님 드릴 말씀이 있습니다 . 어 뭔데 ? 얘기해 제가 다음 주에 급하게 집에...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>이거 누가 올린 보고서야 ! 오늘 까지 올리라고 하셔서 제가 아침에 올려놨는데요 ....</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>왜 내가 부탁한 돈 아직도 안보냈어 ? 나 돈 없는거 알잖아 ? 그럼 우리 니가 보...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5081</th>\n",
       "      <td>부장님 무슨일입니까 ? 아 이번에 제출한 연차 다음으로 미뤄 . 죄송한데 그날은 제...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>야 너 내가 카톡하면 초 안에 보라고 했지 아 . 어 . 미안 근데 왜이렇게 늦어 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9480 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  class\n",
       "idx                                                           \n",
       "3709  대리님 혹시 자료 보내주실 수 있을까요 ? 자료가 부족해서요 네 ? 나 알맞게 보냈...      3\n",
       "5698  김회장 ! 오랜만이야 . 내가 놀라운 소식을 하나 들고 왔는데 말이야 . 지금 바쁘...      0\n",
       "3094  또 뭘 처먹냐 뭐가 . 그만 좀 쳐먹어 씹돼지야 왜그래 . 너 걸어다닐때마다 바닥이...      1\n",
       "255   지금 뭐하는건가 ? 꼼짝마 손들어 내가 그럴 것 같니 ? 딸에게 독약이라도 줄 거야...      4\n",
       "3314  내가 술 좀 그만 먹으라고 했지 ? 죽을래 ? 술을 먹든 말든 신경 꺼 네가 술 먹...      4\n",
       "...                                                 ...    ...\n",
       "3608  저 책임님 드릴 말씀이 있습니다 . 어 뭔데 ? 얘기해 제가 다음 주에 급하게 집에...      3\n",
       "332   이거 누가 올린 보고서야 ! 오늘 까지 올리라고 하셔서 제가 아침에 올려놨는데요 ....      3\n",
       "2541  왜 내가 부탁한 돈 아직도 안보냈어 ? 나 돈 없는거 알잖아 ? 그럼 우리 니가 보...      4\n",
       "5081  부장님 무슨일입니까 ? 아 이번에 제출한 연차 다음으로 미뤄 . 죄송한데 그날은 제...      3\n",
       "2223  야 너 내가 카톡하면 초 안에 보라고 했지 아 . 어 . 미안 근데 왜이렇게 늦어 ...      1\n",
       "\n",
       "[9480 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data 증강\n",
    "\n",
    "def random_deletion(words, p=0.3):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "\n",
    "    if len(new_words) == 0:\n",
    "        rand_int = random.randint(0, len(words) - 1)\n",
    "        return [words[rand_int]]\n",
    "\n",
    "    return \"\".join(new_words)\n",
    "\n",
    "def swap_word(new_words):\n",
    "    random_idx_1 = random.randint(0, len(new_words) - 1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "\n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words) - 1)\n",
    "        counter += 1\n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "\n",
    "    new_words[random_idx_1], new_words[random_idx_2] = (\n",
    "        new_words[random_idx_2],\n",
    "        new_words[random_idx_1],\n",
    "    )\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def random_swap(words, n=3):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "\n",
    "    return new_words\n",
    "\n",
    "\n",
    "print(\"before data augmentation: \", len(train_sentences))\n",
    "\n",
    "train_splted = pd.DataFrame({ \"sentence\": train_sentences, \"class\": train_labels })\n",
    "\n",
    "train_splted = train_splted[train_splted[\"class\"] != 2]\n",
    "\n",
    "# random deletion\n",
    "train_splted_rd = train_splted.copy()\n",
    "train_splted_rd[\"sentence\"] = train_splted_rd[\"sentence\"].apply(random_deletion)\n",
    "\n",
    "# random swap\n",
    "train_splted_rs = train_splted.copy()\n",
    "\n",
    "# with data augmentation\n",
    "train_concated = pd.concat([train_splted , train_splted_rd , train_splted_rs])\n",
    "\n",
    "print(\"after data augmentation: \", len(train_concated))\n",
    "\n",
    "train_concated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "35491bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9480\n"
     ]
    }
   ],
   "source": [
    "train_sentences, train_labels = train_concated[\"sentence\"], train_concated[\"class\"]\n",
    "print(len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "44871f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11443\n",
      "11443\n"
     ]
    }
   ],
   "source": [
    "train_sentences = pd.concat([train_sentences, pd.Series(normal_sentences)], axis=\"rows\")\n",
    "train_labels = pd.concat([train_labels, normal_labels], axis=\"rows\")\n",
    "\n",
    "print(len(train_sentences))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "18fef61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3709    대리님 혹시 자료 보내주실 수 있을까요 ? 자료가 부족해서요 네 ? 나 알맞게 보냈...\n",
      "5698    김회장 ! 오랜만이야 . 내가 놀라운 소식을 하나 들고 왔는데 말이야 . 지금 바쁘...\n",
      "3094    또 뭘 처먹냐 뭐가 . 그만 좀 쳐먹어 씹돼지야 왜그래 . 너 걸어다닐때마다 바닥이...\n",
      "255     지금 뭐하는건가 ? 꼼짝마 손들어 내가 그럴 것 같니 ? 딸에게 독약이라도 줄 거야...\n",
      "3314    내가 술 좀 그만 먹으라고 했지 ? 죽을래 ? 술을 먹든 말든 신경 꺼 네가 술 먹...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9d5da272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  class\n",
      "0  영지야 나 토익 학원 다니느라 돈을 다써서 그런데 만원만 빌려주라 응 ? 안돼 . ...      0\n",
      "1  땡땡씨 얼굴 너무 크지않아 ? 맞아요 . 큰얼굴에 화장해놓고 자기가 이쁜줄 안다니까...      3\n",
      "2  야 ! 너네 또 뛰었지 ! 저희 집이 뛴 거 아니라구요 너네잖아 ! 저희 지금 막 ...      4\n",
      "3  야 저기 도움반 애 온다 쟤 진짜 짜증나 저번에 계속 나쫓아다님 지능이 낮아서 그런...      1\n",
      "4  전 비오는날 빗소리 듣기가 너무 좋아요 이상하게 비오는 소리는 부침개 부치는 소리랑...      2\n",
      "                                                sentence  class\n",
      "11438  기 장님 급을언 주시나요알겠어새끼 기다 봐 지금 월급을못 받은 게 일이 넘었요 시는...      3\n",
      "11439  오빠 나 돈 좀 빌려줘 친구가 돈 빌려달래 무슨 친구 얼마 ? 아니 그냥 좀 빌려줘...      0\n",
      "11440  야 찐따야 . ? 지금 나 말하는거야 ? 그럼 너밖에 더 있겠냐 ? 왜 내가 찐따야...      1\n",
      "11441  나 이번에 처음 해외여행 가면서 비행기도 처음 타봤어 어머 그래 ? 고등학교 때 보...      2\n",
      "11442  인생 영화 추천좀 난 배우가 체질인듯 키키 멜로 배우 ? 액션 ? 내 인생 영화는 ...      2\n"
     ]
    }
   ],
   "source": [
    "# shuffle\n",
    "train_sentences = train_sentences.reset_index(drop=True)\n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "\n",
    "train_sentences.name = \"sentence\"\n",
    "train_labels.name = \"class\"\n",
    "\n",
    "concat = pd.concat([train_sentences, train_labels], axis=\"columns\")\n",
    "concat = concat.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(concat.head())\n",
    "print(concat.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584b7bf",
   "metadata": {},
   "source": [
    "## 3. 모델\n",
    "### 3.1-1 토크나이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f6108853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT 토크나이저와 모델 준비\n",
    "model_name = \"klue/bert-base\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "special_tokens_dict = {'additional_special_tokens': ['<EOL>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07263bc7",
   "metadata": {},
   "source": [
    "### 3.1-2 토크나이저 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0ad838fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11443 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 11443/11443 [00:09<00:00, 1154.87it/s]\n",
      "100%|██████████| 990/990 [00:00<00:00, 1110.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋을 BERT 입력 형식으로 변환\n",
    "X_train, y_train = convert_examples_to_features(\n",
    "    concat[\"sentence\"], concat[\"class\"],\n",
    "    max_seq_len=MAX_LEN, tokenizer=tokenizer\n",
    ")\n",
    "X_valid, y_valid = convert_examples_to_features(\n",
    "    val_sentences, val_labels, \n",
    "    max_seq_len=MAX_LEN, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# train_encodings = tokenizer(train_sentences, truncation=True, padding=True, max_length=MAX_LEN) # 뒤쪽에 패딩\n",
    "# val_encodings = tokenizer(val_sentences, truncation=True, padding=True, max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69608c97",
   "metadata": {},
   "source": [
    "### 3.2 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1c552a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertForMultiClassClassification(tf.keras.Model):\n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.1):\n",
    "        super(TFBertForMultiClassClassification, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes,\n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
    "                                                kernel_regularizer=l2(0.01),\n",
    "                                                activation='softmax',\n",
    "                                                name='classifier')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask, token_type_ids = inputs\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "        cls_token = outputs[1]\n",
    "        dropped = self.dropout(cls_token)\n",
    "        prediction = self.classifier(dropped)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "20c91ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForMultiClassClassification(model_name, num_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a18f8",
   "metadata": {},
   "source": [
    "### 3.3 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9db9de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "lr = 5e-5\n",
    "EPOCH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d587f",
   "metadata": {},
   "source": [
    "### 3.4 TF 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67e7733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 데이터셋 생성\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#     dict(X_train),\n",
    "#     y_train\n",
    "# )).shuffle(100).batch(BATCH_SIZE)\n",
    "\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#     dict(X_valid),\n",
    "#     y_valid\n",
    "# )).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca859b",
   "metadata": {},
   "source": [
    "### 3.5 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9bcad578",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d2ca2",
   "metadata": {},
   "source": [
    "### 3.6 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fadc6c0",
   "metadata": {},
   "source": [
    "### 3.6-1 콜백 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0052fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # 검증 손실을 모니터링\n",
    "    patience=2,            # 3 에포크 동안 개선되지 않으면 중지\n",
    "    restore_best_weights=True  # 최상의 가중치를 복원\n",
    ")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "time = now.strftime(\"%y-%m-%d %H:%M\")\n",
    "data_type = 0\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=f'./models/klue_with_aug_weights_{data_type}_m250_{time}.keras',  # 모델 가중치를 저장할 파일 경로\n",
    "    monitor='val_loss',        # 검증 손실을 모니터링\n",
    "    save_best_only=True,       # 최상의 모델만 저장\n",
    "    save_weights_only=True,    # 저장 (가중치)\n",
    "    mode='min',                # 'val_loss'가 최소일 때 저장\n",
    "    verbose=1                  # 저장 시 로그 출력\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9d868",
   "metadata": {},
   "source": [
    "### 3.6-2 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c7e23b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "358/358 [==============================] - 474s 1s/step - loss: 0.4328 - accuracy: 0.8778 - val_loss: 0.3638 - val_accuracy: 0.9111\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36384, saving model to ./models/klue_with_aug_weights_0_m250_24-06-26 15:53.keras\n",
      "Epoch 2/10\n",
      "358/358 [==============================] - 460s 1s/step - loss: 0.1657 - accuracy: 0.9744 - val_loss: 0.4500 - val_accuracy: 0.9020\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.36384\n",
      "Epoch 3/10\n",
      "358/358 [==============================] - 460s 1s/step - loss: 0.1234 - accuracy: 0.9869 - val_loss: 0.5060 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fedf65c82b0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=EPOCH,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb52d49",
   "metadata": {},
   "source": [
    "### 3.7 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e49ee940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 13s 428ms/step - loss: 0.3638 - accuracy: 0.9111\n",
      "평가 결과: [0.3638433516025543, 0.9111111164093018]\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "evaluation = model.evaluate(X_valid, y_valid)\n",
    "print(\"평가 결과:\", evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5b3945fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np \n",
    "\n",
    "def score(model, val):\n",
    "    X, y = val\n",
    "    # 실제 예측값 생성\n",
    "    real_predictions = model.predict(X)\n",
    "\n",
    "    # 예측값을 레이블로 변환\n",
    "    real_predicted_labels = np.argmax(real_predictions, axis=1)\n",
    "\n",
    "    # 정확도 계산\n",
    "    real_accuracy = accuracy_score(y, real_predicted_labels)\n",
    "    print(f\"Real Accuracy: {real_accuracy:.4f}\")\n",
    "\n",
    "    # 분류 보고서 생성\n",
    "    real_report = classification_report(y, real_predicted_labels, target_names=[f\"Class {i}\" for i in range(5)])\n",
    "    print(real_report)\n",
    "\n",
    "    # F1 스코어 계산\n",
    "    real_f1 = f1_score(y, real_predicted_labels, average='weighted')\n",
    "    print(f\"\\nWeighted F1 Score (based on real predictions): {real_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "09a4ec48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Accuracy: 0.9111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.87      0.92      0.89       196\n",
      "     Class 1       0.89      0.81      0.85       219\n",
      "     Class 2       0.96      0.99      0.98       200\n",
      "     Class 3       0.91      0.97      0.94       196\n",
      "     Class 4       0.93      0.87      0.90       179\n",
      "\n",
      "    accuracy                           0.91       990\n",
      "   macro avg       0.91      0.91      0.91       990\n",
      "weighted avg       0.91      0.91      0.91       990\n",
      "\n",
      "\n",
      "Weighted F1 Score (based on real predictions): 0.9101\n"
     ]
    }
   ],
   "source": [
    "score(model, (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a2b96",
   "metadata": {},
   "source": [
    "## 4. 모델 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6061d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_data_path = \"/aiffel/aiffel/dktc/data/test.json\"\n",
    "test = pd.read_json(test_data_path).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "73536474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_predict = []\n",
    "\n",
    "for idx, value in test.iterrows():\n",
    "\n",
    "    test_sentence = value[\"text\"]\n",
    "    \n",
    "    test_encodings = tokenizer(test_sentence, truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"tf\")\n",
    "\n",
    "    test_predictions = model.predict(\n",
    "        (test_encodings[\"input_ids\"],\n",
    "         test_encodings[\"attention_mask\"],\n",
    "         test_encodings[\"token_type_ids\"])\n",
    "    )\n",
    "    test_class_probabilities = tf.nn.softmax(test_predictions, axis=-1).numpy() # [[0.13297564 0.8358507  0.00801584 0.02315779]]\n",
    "    test_predicted_class = np.argmax(test_class_probabilities, axis=1) # [ 1 ]\n",
    "    test_predict.append(test_predicted_class[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c23689bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n",
      "  file_name  class\n",
      "0     t_000      1\n",
      "1     t_001      2\n",
      "2     t_002      2\n",
      "3     t_003      4\n",
      "4     t_004      3\n"
     ]
    }
   ],
   "source": [
    "# {'협박 대화': 4, '갈취 대화': 0, '직장 내 괴롭힘 대화': 3, '기타 괴롭힘 대화': 1, '일반 대화': 2}\n",
    "#   협박 대화 : 0,  갈취 대화 : 1,  직장 내 괴롭힘 대화 : 2,  기타 괴롭힘 대화 : 3,  일반 대화 : 4\n",
    "def labelnum_to_text(x):\n",
    "    if x == 0:\n",
    "        return '01'\n",
    "    if x == 1:\n",
    "        return '03'\n",
    "    if x == 2:\n",
    "        return '04'\n",
    "    if x == 3:\n",
    "        return '02'\n",
    "    if x == 4:\n",
    "        return '00'\n",
    "\n",
    "import datetime\n",
    "    \n",
    "submission = pd.read_csv(\"../data/new_submission.csv\")\n",
    "submission[\"class\"] = [ labelnum_to_text(pred) for pred in test_predict ]\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "filename = now.strftime(\"../sub/submission %y-%m-%d %H:%M.csv\")\n",
    "\n",
    "submission.to_csv(filename, index=False)\n",
    "submit_file = pd.read_csv(filename)\n",
    "\n",
    "print(submit_file.shape)\n",
    "print(submit_file.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f8797e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
