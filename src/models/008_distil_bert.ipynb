{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5483da0f",
   "metadata": {},
   "source": [
    "## 0. 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c297de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFDistilBertForSequenceClassification, DistilBertTokenizer, TFDistilBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import json\n",
    "from soynlp.normalizer import *\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e913abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug(x,y,classs):\n",
    "    def random_deletion(words, p=0.1):\n",
    "        if len(words) == 1:\n",
    "            return words\n",
    "\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            r = random.uniform(0, 1)\n",
    "            if r > p:\n",
    "                new_words.append(word)\n",
    "\n",
    "        if len(new_words) == 0:\n",
    "            rand_int = random.randint(0, len(words)-1)\n",
    "            return [words[rand_int]]\n",
    "\n",
    "        return ''.join(new_words)\n",
    "\n",
    "    def swap_word(new_words):\n",
    "        n = 5\n",
    "        for _ in range(n):\n",
    "            random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "            random_idx_2 = random_idx_1\n",
    "            counter = 0\n",
    "\n",
    "            while random_idx_2 == random_idx_1:\n",
    "                random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "                counter += 1\n",
    "                if counter > 3:\n",
    "                    return new_words\n",
    "\n",
    "            new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "        return ' '.join(new_words)\n",
    "\n",
    "    def random_swap(words):\n",
    "        new_words = list()\n",
    "        for word in words:\n",
    "            new_words.append(swap_word(word.split()))\n",
    "\n",
    "        return new_words\n",
    "    df = pd.concat([x,y],axis=1).reset_index(drop=True)\n",
    "    df_rd = df[df['class']==classs].copy()\n",
    "    df_rd['conversation'] = df_rd['conversation'].apply(random_deletion)\n",
    "    df_rs = df[df['class']==classs].copy()\n",
    "    df_rs['conversation'] = random_swap(df_rs['conversation'].values)\n",
    "    \n",
    "    df_concated = pd.concat([df, df_rs])\n",
    "    return df_concated.loc[:,['conversation']] , df_concated['class']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211bfd0",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "21fefbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>일반 대화</td>\n",
       "      <td>야 여행 가기 너무 좋은 날씨다\\n날씨 진짜 너무 좋아 **이는 일하기 싫을 날씨야...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>사정이 있었어요 . 살려주세요 . 뒤진 사람들 중에 사정 없는 사람 없어 맞아 . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>배고프다 그러게 야 만원만 빌려줘봐 오늘도 ? 저번에도 만원 빌려가고 안 돌려 줬잖...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기 실례지만 제가 휠체어를 타는 장애인이라서 아 혼자 휠체어 운전하시는게 어렵다고...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>김 사원 낙하산이라는 소문이 있던데 ? 요즘 시대에도 낙하산이 있어요 ? 요즘 더 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4826</th>\n",
       "      <td>일반 대화</td>\n",
       "      <td>티비는 잘 안 보니?\\n티비 매일 보지\\n너는?\\n키키 요즘 핫한 스우파 안 보니?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4827</th>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>이거 나이키 신상이네 ? 응 아빠가 사주셨어 우와 멋지네 근데 너랑은 별로 안어울린...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4828</th>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>일처리를 어떻게 한거야 ! 기간 내에 상품이 못오면 그사이 손실은 어떻게 할거냐고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4829</th>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>어우 정대리 이번에도 참 글래머 스럽게 입고왔네 네 ? 지금 저한태 하신말이세요 ?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4830</th>\n",
       "      <td>협박 대화</td>\n",
       "      <td>전화 좀 그만하세요 . 당신이 배달앱 리뷰에 내 욕을 써놔서 내가 잘 다니던 직장에...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4830 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            class                                       conversation\n",
       "idx                                                                 \n",
       "1           일반 대화  야 여행 가기 너무 좋은 날씨다\\n날씨 진짜 너무 좋아 **이는 일하기 싫을 날씨야...\n",
       "2           갈취 대화  사정이 있었어요 . 살려주세요 . 뒤진 사람들 중에 사정 없는 사람 없어 맞아 . ...\n",
       "3           갈취 대화  배고프다 그러게 야 만원만 빌려줘봐 오늘도 ? 저번에도 만원 빌려가고 안 돌려 줬잖...\n",
       "4           갈취 대화  저기 실례지만 제가 휠체어를 타는 장애인이라서 아 혼자 휠체어 운전하시는게 어렵다고...\n",
       "5     직장 내 괴롭힘 대화  김 사원 낙하산이라는 소문이 있던데 ? 요즘 시대에도 낙하산이 있어요 ? 요즘 더 ...\n",
       "...           ...                                                ...\n",
       "4826        일반 대화  티비는 잘 안 보니?\\n티비 매일 보지\\n너는?\\n키키 요즘 핫한 스우파 안 보니?...\n",
       "4827        갈취 대화  이거 나이키 신상이네 ? 응 아빠가 사주셨어 우와 멋지네 근데 너랑은 별로 안어울린...\n",
       "4828  직장 내 괴롭힘 대화  일처리를 어떻게 한거야 ! 기간 내에 상품이 못오면 그사이 손실은 어떻게 할거냐고 ...\n",
       "4829  직장 내 괴롭힘 대화  어우 정대리 이번에도 참 글래머 스럽게 입고왔네 네 ? 지금 저한태 하신말이세요 ?...\n",
       "4830        협박 대화  전화 좀 그만하세요 . 당신이 배달앱 리뷰에 내 욕을 써놔서 내가 잘 다니던 직장에...\n",
       "\n",
       "[4830 rows x 2 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path =\"~/aiffel/dktc/data2/train0.csv\"\n",
    "train_data = pd.read_csv(train_data_path,index_col=0)\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f158d",
   "metadata": {},
   "source": [
    "## 2. 데이터 준비 (Data preparation)\n",
    "### 2.1-1 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b0cb904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # synolp\n",
    "    emoticon_normalize(sentence)\n",
    "    repeat_normalize(sentence)\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    # base preprocess\n",
    "    sentence = re.sub(r'([^a-zA-Zㄱ-ㅎ가-힣?.!,])', \" \", sentence)\n",
    "    sentence = re.sub(r'!+', '!', sentence)\n",
    "    sentence = re.sub(r'\\?+', '?', sentence)\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # 엔터 구분 (\\n)\n",
    "    sentence = sentence.replace(\"\\n\", \" \")\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b887e142",
   "metadata": {},
   "source": [
    "### 2.1-2 전처리 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4ad2f592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4830/4830 [00:01<00:00, 3205.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# 학습할 문장이 담길 배열\n",
    "sentences = []\n",
    "\n",
    "for val in tqdm(train_data['conversation']):\n",
    "    sentences.append(preprocess_sentence(val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b935b19",
   "metadata": {},
   "source": [
    "### 2.2 최대 길이 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "98cd3de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44221a",
   "metadata": {},
   "source": [
    "### 2.3 class(label) 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2f6f179b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4830"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "CLASS_NAMES = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화','일반 대화']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(CLASS_NAMES)\n",
    "\n",
    "train_data['class'] = encoder.transform(train_data['class'])\n",
    "labels = train_data['class']\n",
    "\n",
    "len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d3ff445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: {'협박 대화': 4, '갈취 대화': 0, '직장 내 괴롭힘 대화': 3, '기타 괴롭힘 대화': 1, '일반 대화': 2}\n"
     ]
    }
   ],
   "source": [
    "class_mapping = {class_name: encoder.transform([class_name])[0] for class_name in CLASS_NAMES}\n",
    "print(\"Class mapping:\", class_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccddef3e",
   "metadata": {},
   "source": [
    "### 2.4 train-val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b6b1c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625526f",
   "metadata": {},
   "source": [
    "### 2.5 데이터 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4f4eaee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sentences, train_labels = aug(train_sentences, train_labels, 0)\n",
    "# train_sentences, train_labels = aug(train_sentences, train_labels, 1)\n",
    "# train_sentences, train_labels = aug(train_sentences, train_labels, 2)\n",
    "# train_sentences, train_labels = aug(train_sentences, train_labels, 3)\n",
    "# train_sentences, train_labels = aug(train_sentences, train_labels, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e70eafd",
   "metadata": {},
   "source": [
    "## 3. 모델\n",
    "### 3.1-1 토크나이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ddeb244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 토크나이저와 모델 준비\n",
    "\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca20d23a",
   "metadata": {},
   "source": [
    "### 3.1-2 토크나이저 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "daaf1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 BERT 입력 형식으로 변환\n",
    "train_encodings = tokenizer(train_sentences, truncation=True, padding=True, max_length=MAX_LEN) # 뒤쪽에 패딩\n",
    "val_encodings = tokenizer(val_sentences, truncation=True, padding=True, max_length=MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fafeba",
   "metadata": {},
   "source": [
    "### 3.2 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8ebbece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_96']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#model = TFBertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=5)\n",
    "\n",
    "#model = TFDistilBertModel.from_pretrained(\"distilbert-base-multilingual-cased\", num_labels=5)\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-multilingual-cased\", num_labels=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e10c1a",
   "metadata": {},
   "source": [
    "### 3.3 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "97d04664",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "lr = 5e-5\n",
    "EPOCH = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8dff86",
   "metadata": {},
   "source": [
    "### 3.4 TF 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1f0e2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 데이터셋 생성\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    ")).shuffle(100).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    ")).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a883ed",
   "metadata": {},
   "source": [
    "### 3.5 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b99ca646",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92243c32",
   "metadata": {},
   "source": [
    "### 3.6 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31b413",
   "metadata": {},
   "source": [
    "### 3.6-1 콜백 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0c0a8159",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # 검증 손실을 모니터링\n",
    "    patience=2,            # 3 에포크 동안 개선되지 않으면 중지\n",
    "    restore_best_weights=True  # 최상의 가중치를 복원\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model_weights.h5',  # 모델 가중치를 저장할 파일 경로\n",
    "    monitor='val_loss',        # 검증 손실을 모니터링\n",
    "    save_best_only=True,       # 최상의 모델만 저장\n",
    "    save_weights_only=True,   # 저장 (가중치)\n",
    "    mode='min',                # 'val_loss'가 최소일 때 저장\n",
    "    verbose=1                  # 저장 시 로그 출력\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd04c4",
   "metadata": {},
   "source": [
    "### 3.6-2 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c0989f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "242/242 [==============================] - 158s 626ms/step - loss: 1.0645 - accuracy: 0.5499 - val_loss: 0.7336 - val_accuracy: 0.7308\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73355, saving model to best_model_weights.h5\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 149s 616ms/step - loss: 0.5705 - accuracy: 0.7961 - val_loss: 0.4308 - val_accuracy: 0.8447\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73355 to 0.43076, saving model to best_model_weights.h5\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 149s 616ms/step - loss: 0.3714 - accuracy: 0.8763 - val_loss: 0.4306 - val_accuracy: 0.8458\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43076 to 0.43061, saving model to best_model_weights.h5\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 150s 618ms/step - loss: 0.2615 - accuracy: 0.9154 - val_loss: 0.4718 - val_accuracy: 0.8416\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.43061\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 149s 615ms/step - loss: 0.1790 - accuracy: 0.9436 - val_loss: 0.4824 - val_accuracy: 0.8644\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.43061\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 149s 616ms/step - loss: 0.1386 - accuracy: 0.9557 - val_loss: 0.5069 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.43061\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 149s 615ms/step - loss: 0.0850 - accuracy: 0.9728 - val_loss: 0.5588 - val_accuracy: 0.8561\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43061\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 149s 616ms/step - loss: 0.0683 - accuracy: 0.9785 - val_loss: 0.5300 - val_accuracy: 0.8685\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43061\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 149s 615ms/step - loss: 0.0846 - accuracy: 0.9752 - val_loss: 0.6827 - val_accuracy: 0.8478\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43061\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 149s 615ms/step - loss: 0.0647 - accuracy: 0.9819 - val_loss: 0.6303 - val_accuracy: 0.8602\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x78b2f4e6d730>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCH,\n",
    "#     callbacks=[early_stopping, checkpoint]\n",
    "    callbacks=[checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b665f2",
   "metadata": {},
   "source": [
    "### 3.7 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7b68ba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 11s 182ms/step - loss: 0.6303 - accuracy: 0.8602\n",
      "평가 결과: [0.6302611827850342, 0.8602484464645386]\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "evaluation = model.evaluate(val_dataset)\n",
    "print(\"평가 결과:\", evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fa5a496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def score(model, val):\n",
    "    X, y = [], []\n",
    "    for batch in val_dataset:\n",
    "        inputs, labels = batch\n",
    "        X.append(inputs)\n",
    "        y.append(labels)\n",
    "    # 각 입력 키에 대해 데이터를 결합하여 numpy 배열로 변환\n",
    "    X = {key: np.concatenate([d[key].numpy() for d in X], axis=0) for key in X[0].keys()}\n",
    "    y = np.concatenate(y, axis=0)\n",
    "    \n",
    "    # 실제 예측값 생성\n",
    "    real_predictions = model.predict(X)\n",
    "    logits = real_predictions.logits\n",
    "\n",
    "    # 예측값을 레이블로 변환\n",
    "    if logits.ndim > 1:\n",
    "        real_predicted_labels = np.argmax(logits, axis=1)\n",
    "    else:\n",
    "        real_predicted_labels = (logits > 0.5).astype(int)\n",
    "    \n",
    "\n",
    "    # 정확도 계산\n",
    "    real_accuracy = accuracy_score(y, real_predicted_labels)\n",
    "    print(f\"Real Accuracy: {real_accuracy:.4f}\")\n",
    "\n",
    "    # 분류 보고서 생성\n",
    "    real_report = classification_report(y, real_predicted_labels, target_names=[f\"Class {i}\" for i in range(logits.shape[1])])\n",
    "    print(real_report)\n",
    "\n",
    "    # F1 스코어 계산\n",
    "    real_f1 = f1_score(y, real_predicted_labels, average='weighted')\n",
    "    print(f\"\\nWeighted F1 Score (based on real predictions): {real_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ffcc94a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Accuracy: 0.8602\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.73      0.84      0.78       175\n",
      "     Class 1       0.76      0.87      0.81       215\n",
      "     Class 2       1.00      0.94      0.97       178\n",
      "     Class 3       0.93      0.93      0.93       201\n",
      "     Class 4       0.95      0.72      0.82       197\n",
      "\n",
      "    accuracy                           0.86       966\n",
      "   macro avg       0.87      0.86      0.86       966\n",
      "weighted avg       0.87      0.86      0.86       966\n",
      "\n",
      "\n",
      "Weighted F1 Score (based on real predictions): 0.8616\n"
     ]
    }
   ],
   "source": [
    "score(model, val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f4537",
   "metadata": {},
   "source": [
    "## 4. 모델 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9d527b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path =\"/aiffel/aiffel/dktc/data2/test.json\"\n",
    "\n",
    "with open(test_data_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    test = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2ed56a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_predicst = list()\n",
    "\n",
    "for key in test:\n",
    "    \n",
    "    test_sentence = test[key]['text']\n",
    "    inputs = tokenizer(test_sentence, truncation=True, padding=True, max_length=300, return_tensors=\"tf\")\n",
    "    test_predictions = model.predict(inputs.data) \n",
    "    test_class_probabilities = tf.nn.softmax(test_predictions.logits, axis=-1).numpy()\n",
    "    test_predicted_class = np.argmax(test_class_probabilities, axis=1)\n",
    "    test_predicst.append(test_predicted_class[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5ef0b066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predicst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "06d5d345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t_000</th>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_001</th>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_002</th>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_003</th>\n",
       "      <td>03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_004</th>\n",
       "      <td>03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_495</th>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_496</th>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_497</th>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_498</th>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_499</th>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class\n",
       "t_000    01\n",
       "t_001    02\n",
       "t_002    02\n",
       "t_003    03\n",
       "t_004    03\n",
       "...     ...\n",
       "t_495    02\n",
       "t_496    01\n",
       "t_497    01\n",
       "t_498    00\n",
       "t_499    01\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def labelnum_to_text(x):\n",
    "    if x == 1 : # '기타 괴롭힘 대화'\n",
    "        return '03'\n",
    "    if x == 2 : # '일반 대화'\n",
    "        return '04'\n",
    "    if x == 3 : # '직장 내 괴롭힘 대화\n",
    "        return '02'\n",
    "    if x == 0 : # '갈취 대화' \n",
    "        return '01'\n",
    "    if x == 4 : # '협박 대화'\n",
    "        return \"00\"\n",
    "    \n",
    "submission = pd.DataFrame({'class':test_predicst}, index=list(test.keys()))\n",
    "submission['class'] = submission['class'].apply(labelnum_to_text)\n",
    "submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c670ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('~/aiffel/dktc/data2/submission_0.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1edcde5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60b294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d7d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
