{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f13c13",
   "metadata": {},
   "source": [
    "## 0. 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4425cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import json\n",
    "from soynlp.normalizer import *\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed4fb05",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "98fe7d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>일반 대화</td>\n",
       "      <td>오늘 날씨 어때?\\n맑고 따뜻해.\\n좋네! 주말에 계획 있어?\\n등산 갈 생각이야....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>일반 대화</td>\n",
       "      <td>오늘 날씨 어때?\\n맑고 따뜻해.\\n좋네! 주말에 계획 있어?\\n등산 갈 생각이야....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 피아노 좀 쳐봐.\\n싫어.\\n왜 손가락 없다고 유세 떠는 거야?\\n이씨.\\n비행...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>요즘에는 액수가 작네?\\n미안해 요즘에 용돈이 작아\\n그게 나랑 무슨 상관이야?\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>야 이거봐 완전 길동이 닯음\\n 진짜네 \\n야 그러지마.\\n왜 똑같구만 원숭이 \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4946</th>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>어이 신병.\\n이병 김범례.\\n와봐.\\n네.\\n네? 뒤질래?\\n.자.잘못들었습니다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947</th>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>오늘 회의 안건인 길동프로그램의 출연자는 누구로 할 것인가에 대해 모두 의견 내주시...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>야 열심히들 해라 새끼들아\\n넵 감사해요 부장님\\n야 내가 언제 너 한테 말했냐\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>일반 대화</td>\n",
       "      <td>오늘 날씨 어때?\\n맑고 따뜻해.\\n좋네! 주말에 계획 있어?\\n등산 갈 생각이야....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>야 신입 여기 와 봐.\\n네?\\n이게 선배가 부르는데 말꼬리 올려? 너 커피 탈 줄...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4950 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            class                                       conversation\n",
       "idx                                                                 \n",
       "1           일반 대화  오늘 날씨 어때?\\n맑고 따뜻해.\\n좋네! 주말에 계획 있어?\\n등산 갈 생각이야....\n",
       "2           일반 대화  오늘 날씨 어때?\\n맑고 따뜻해.\\n좋네! 주말에 계획 있어?\\n등산 갈 생각이야....\n",
       "3       기타 괴롭힘 대화  너 피아노 좀 쳐봐.\\n싫어.\\n왜 손가락 없다고 유세 떠는 거야?\\n이씨.\\n비행...\n",
       "4           갈취 대화  요즘에는 액수가 작네?\\n미안해 요즘에 용돈이 작아\\n그게 나랑 무슨 상관이야?\\n...\n",
       "5       기타 괴롭힘 대화  야 이거봐 완전 길동이 닯음\\n 진짜네 \\n야 그러지마.\\n왜 똑같구만 원숭이 \\n...\n",
       "...           ...                                                ...\n",
       "4946  직장 내 괴롭힘 대화  어이 신병.\\n이병 김범례.\\n와봐.\\n네.\\n네? 뒤질래?\\n.자.잘못들었습니다?...\n",
       "4947  직장 내 괴롭힘 대화  오늘 회의 안건인 길동프로그램의 출연자는 누구로 할 것인가에 대해 모두 의견 내주시...\n",
       "4948  직장 내 괴롭힘 대화  야 열심히들 해라 새끼들아\\n넵 감사해요 부장님\\n야 내가 언제 너 한테 말했냐\\n...\n",
       "4949        일반 대화  오늘 날씨 어때?\\n맑고 따뜻해.\\n좋네! 주말에 계획 있어?\\n등산 갈 생각이야....\n",
       "4950  직장 내 괴롭힘 대화  야 신입 여기 와 봐.\\n네?\\n이게 선배가 부르는데 말꼬리 올려? 너 커피 탈 줄...\n",
       "\n",
       "[4950 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path =\"~/aiffel/dktc/data2/train1.csv\"\n",
    "train_data = pd.read_csv(train_data_path,index_col=0)\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5a58b",
   "metadata": {},
   "source": [
    "## 2. 데이터 준비 (Data preparation)\n",
    "### 2.1-1 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a60da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    emoticon_normalize(sentence)\n",
    "    repeat_normalize(sentence)\n",
    "    sentence = re.sub(r'([^a-zA-Zㄱ-ㅎ가-힣?.!,])', \" \", sentence)\n",
    "    sentence = re.sub(r'!+', '!', sentence)\n",
    "    sentence = re.sub(r'\\?+', '?', sentence)\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf94aae",
   "metadata": {},
   "source": [
    "### 2.1-2 전처리 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e9f9718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4950/4950 [00:01<00:00, 3301.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# 학습할 문장이 담길 배열\n",
    "sentences = []\n",
    "\n",
    "for val in tqdm(train_data['conversation']):\n",
    "    sentences.append(preprocess_sentence(val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da8283",
   "metadata": {},
   "source": [
    "### 2.2 최대 길이 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a5fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25992a8f",
   "metadata": {},
   "source": [
    "### 2.3 class(label) 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e61445a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "CLASS_NAMES = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화','일반 대화']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(CLASS_NAMES)\n",
    "\n",
    "train_data['class'] = encoder.transform(train_data['class'])\n",
    "labels = train_data['class']\n",
    "\n",
    "len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931dff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {class_name: encoder.transform([class_name])[0] for class_name in CLASS_NAMES}\n",
    "print(\"Class mapping:\", class_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0f952",
   "metadata": {},
   "source": [
    "### 2.4 train-val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78896ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31043692",
   "metadata": {},
   "source": [
    "## 3. 모델\n",
    "### 3.1-1 토크나이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31475384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 토크나이저와 모델 준비\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d6a49",
   "metadata": {},
   "source": [
    "### 3.1-2 토크나이저 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33d8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 BERT 입력 형식으로 변환\n",
    "train_encodings = tokenizer(\n",
    "    train_sentences, truncation=True, padding=True, max_length=MAX_LEN\n",
    ")  # 뒤쪽에 패딩\n",
    "val_encodings = tokenizer(\n",
    "    val_sentences, truncation=True, padding=True, max_length=MAX_LEN\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd53c22e",
   "metadata": {},
   "source": [
    "### 3.2 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d461df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\", num_labels=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d90d7f",
   "metadata": {},
   "source": [
    "### 3.3 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3534b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "lr = 5e-5\n",
    "EPOCH = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f72774",
   "metadata": {},
   "source": [
    "### 3.4 TF 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcce8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 데이터셋 생성\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))\n",
    "    .shuffle(100)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (dict(val_encodings), val_labels)\n",
    ").batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb17bfd",
   "metadata": {},
   "source": [
    "### 3.5 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f498df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c94a43c",
   "metadata": {},
   "source": [
    "### 3.6 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b00634",
   "metadata": {},
   "source": [
    "### 3.6-1 콜백 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74404f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # 검증 손실을 모니터링\n",
    "    patience=3,            # 3 에포크 동안 개선되지 않으면 중지\n",
    "    restore_best_weights=True  # 최상의 가중치를 복원\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model_weights.h5',  # 모델 가중치를 저장할 파일 경로\n",
    "    monitor='val_loss',        # 검증 손실을 모니터링\n",
    "    save_best_only=True,       # 최상의 모델만 저장\n",
    "    save_weights_only=True,   # 저장 (가중치)\n",
    "    mode='min',                # 'val_loss'가 최소일 때 저장\n",
    "    verbose=1                  # 저장 시 로그 출력\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeef405",
   "metadata": {},
   "source": [
    "### 3.6-2 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eb8c8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "248/248 [==============================] - 186s 750ms/step - loss: 0.5683 - accuracy: 0.8005 - val_loss: 0.5668 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56680, saving model to best_model_weights.h5\n",
      "Epoch 2/10\n",
      "248/248 [==============================] - 186s 751ms/step - loss: 0.4551 - accuracy: 0.8414 - val_loss: 0.4392 - val_accuracy: 0.8465\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56680 to 0.43918, saving model to best_model_weights.h5\n",
      "Epoch 3/10\n",
      "248/248 [==============================] - 186s 752ms/step - loss: 0.3632 - accuracy: 0.8775 - val_loss: 0.4361 - val_accuracy: 0.8444\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43918 to 0.43609, saving model to best_model_weights.h5\n",
      "Epoch 4/10\n",
      "248/248 [==============================] - 186s 751ms/step - loss: 0.2754 - accuracy: 0.9139 - val_loss: 0.4817 - val_accuracy: 0.8505\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.43609\n",
      "Epoch 5/10\n",
      "248/248 [==============================] - 186s 750ms/step - loss: 0.2124 - accuracy: 0.9348 - val_loss: 0.5087 - val_accuracy: 0.8515\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.43609\n",
      "Epoch 6/10\n",
      "248/248 [==============================] - 186s 749ms/step - loss: 0.2240 - accuracy: 0.9270 - val_loss: 0.4638 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.43609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ceaaca9a310>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCH,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1c771b",
   "metadata": {},
   "source": [
    "### 3.7 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39bd0efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 14s 217ms/step - loss: 0.4361 - accuracy: 0.8444\n",
      "평가 결과: [0.4360905885696411, 0.8444444537162781]\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "evaluation = model.evaluate(val_dataset)\n",
    "print(\"평가 결과:\", evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd78c5b",
   "metadata": {},
   "source": [
    "## 4. 모델 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7b7c9564",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/aiffel/dktc/data2/test.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12093/1310437528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_data_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"~/aiffel/dktc/data2/test.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/aiffel/dktc/data2/test.json'"
     ]
    }
   ],
   "source": [
    "test_data_path = \"~/aiffel/dktc/data2/test.json\"\n",
    "\n",
    "with open(test_data_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    test = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86270721",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12093/2082288350.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtest_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     test_predictions = model.predict({\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2377\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2378\u001b[0m                 \u001b[0;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2379\u001b[0m                 \u001b[0;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_predicst = list()\n",
    "\n",
    "for key in test:\n",
    "    test_sentence = test['text']\n",
    "    \n",
    "    test_encodings = tokenizer(test_sentence, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
    "    \n",
    "    test_predictions = model.predict({\n",
    "        \"input_ids\": test_encodings[\"input_ids\"],\n",
    "        \"token_type_ids\": test_encodings[\"token_type_ids\"],\n",
    "        \"attention_mask\": test_encodings[\"attention_mask\"]\n",
    "    }) # [ 0.7805823,  2.6188664, -2.0281641, -0.9672525]\n",
    "    test_class_probabilities = tf.nn.softmax(test_predictions.logits, axis=-1).numpy() # [[0.13297564 0.8358507  0.00801584 0.02315779]]\n",
    "    test_predicted_class = np.argmax(test_class_probabilities, axis=1) # [ 1 ]\n",
    "    test_predicst.append(test_predicted_class[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6395c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelnum_to_text(x):\n",
    "    if x == 1 : # 갈취\n",
    "        return '01'\n",
    "    if x == 2 : # 직장\n",
    "        return '02'\n",
    "    if x == 3 : # 기타\n",
    "        return '03'\n",
    "    if x == 0 : # 협박 \n",
    "        return '00'\n",
    "    \n",
    "submission = pd.DataFrame({'class':test_predicst}, index=list(test.keys()))\n",
    "\n",
    "submission['class'] = submission['class'].apply(labelnum_to_text)\n",
    "submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f4281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  submission.to_csv('~/aiffel/dktc/sub/base_sub.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a646c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56406235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c23739b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120abb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
